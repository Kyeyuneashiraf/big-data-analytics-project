{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Individual Assessment - Comprehensive Analysis of the UNSW-NB15 Dataset\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyhive import hive\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count\n",
    "\n",
    "# Function to execute a Hive query and return the result as a Pandas DataFrame\n",
    "def execute_hive_query(query, conn):\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "    return pd.DataFrame(result, columns=columns)\n",
    "\n",
    "# Establish Hive connection\n",
    "hive_conn = hive.Connection(host='localhost', port=10000, username='hadoop')\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UNSW-NB15 Individual Assessment\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset into a Spark DataFrame\n",
    "data_path = '../data/UNSW-NB15.csv'  # Adjust the path if necessary\n",
    "df_spark = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Hive Query 1: Average destination bytes (dbytes) for each service type\n",
    "query1 = \"\"\"\n",
    "SELECT service, AVG(dbytes) AS avg_dbytes\n",
    "FROM unsw_nb15\n",
    "GROUP BY service\n",
    "\"\"\"\n",
    "hive_result1 = execute_hive_query(query1, hive_conn)\n",
    "print(hive_result1)\n",
    "\n",
    "# Visualize average destination bytes for each service type\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='service', y='avg_dbytes', data=hive_result1)\n",
    "plt.title('Average Destination Bytes for Each Service Type')\n",
    "plt.xlabel('Service Type')\n",
    "plt.ylabel('Average Destination Bytes')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Spark Query 1: Top 10 destination IPs by the number of connections\n",
    "top_dst_ips = df_spark.groupBy(\"dstip\").agg(count(\"*\").alias(\"connections\")) \\\n",
    "    .orderBy(col(\"connections\").desc()).limit(10).toPandas()\n",
    "print(top_dst_ips)\n",
    "\n",
    "# Visualize the top 10 destination IPs by the number of connections\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='connections', y='dstip', data=top_dst_ips)\n",
    "plt.title('Top 10 Destination IPs by Number of Connections')\n",
    "plt.xlabel('Number of Connections')\n",
    "plt.ylabel('Destination IP')\n",
    "plt.show()\n",
    "\n",
    "# Hive Query 2: Distribution of labels by protocol type\n",
    "query2 = \"\"\"\n",
    "SELECT proto, label, COUNT(*) AS count\n",
    "FROM unsw_nb15\n",
    "GROUP BY proto, label\n",
    "ORDER BY proto, label\n",
    "\"\"\"\n",
    "hive_result2 = execute_hive_query(query2, hive_conn)\n",
    "print(hive_result2)\n",
    "\n",
    "# Visualize the distribution of labels by protocol type\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='proto', y='count', hue='label', data=hive_result2)\n",
    "plt.title('Distribution of Labels by Protocol Type')\n",
    "plt.xlabel('Protocol Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Spark Query 2: Average packet size for each label\n",
    "avg_pkt_size = df_spark.groupBy(\"label\").agg(avg(\"sttl\").alias(\"avg_sttl\"), avg(\"dttl\").alias(\"avg_dttl\")).toPandas()\n",
    "print(avg_pkt_size)\n",
    "\n",
    "# Visualize the average packet size for each label\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_pkt_size_melted = avg_pkt_size.melt(id_vars=\"label\", var_name=\"type\", value_name=\"average\")\n",
    "sns.barplot(x='label', y='average', hue='type', data=avg_pkt_size_melted)\n",
    "plt.title('Average Packet Size (TTL) for Each Label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Average TTL')\n",
    "plt.show()\n",
    "\n",
    "# Close the Hive connection and stop the Spark session\n",
    "hive_conn.close()\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
